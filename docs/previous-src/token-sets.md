# token sets

- measure of ambiguity
- summary of the lexicon
- summary of the paradigms


Up to corpus manager to map this out to occurrences rather than unique tokens.

Or allow corpus manager to feed dupe tokens and do the work the hard way of repeatedly reanlayzing tokens?   (Defines Zipf's law, but if you've got machine time why not?)
